## 论文中的数据收集方式

<figure style="text-align: center;">
  <img src="https://raw.githubusercontent.com/ranyu0802/imgrepo/main/assets/test_picgo7/diffuseloco_data_collect.png" width="50%">
  <figcaption>DiffuseLoco里的数据收集方式</figcaption>
</figure>



## 在`Legged_gym`中收集数据的问题
> [!NOTE]  
> 使用`legged_gym`默认的设置训练RL Policy后，会碰到以下两个问题：

1. 每次重置环境时, `Robot` 会从一个离地面一定高度的位置开始掉落, 这会导致初始阶段的观测值异常。
   + 因为Quaruped要在复杂地形上行走，初始化的时候，需要离地面一定高度.
   + <u>**现在在只需要在平地上运动**</u>
      + \[SOLUTION\]所以把`Robot`的初始位置调到离地面较近的位置，避免掉落带来的异常观测值。

2. 每次重新采样速度指令的时候，`lin_vel_x`, `lin_vel_y` 和 `ang_vel_yaw` 是单独采样。
   + 采样的时候，速度的分布要均匀覆盖整个指令空间。
   + 并且会把采集到的较小的速度指令置零。

下图是使用原始的`legged_gym`采样方法收集的数据中速度指令的分布情况：
![command distribution using original legged_gym sampling method](https://raw.githubusercontent.com/ranyu0802/imgrepo/main/assets/test_picgo7/cmd_dis.png)


## `legged_gym`中数据收集的修改




### 代码相关
`Config`中的设置
```python
class env:
   episode_length_s = 20
# decimation: Number of control action updates @ sim DT per policy DT
class control:
   decimation = 4
class sim:
   dt =  0.005
class commands:
   resampling_time = 10. # time before command are changed[s]
```

`legged_robot.py`中的`velocity command`重新采样

```python
# Simulation Time Step
self.dt = self.cfg.control.decimation * self.sim_params.dt # 0.02s

# 在legged_robot.py 中  _post_physics_step_callback
env_ids = (self.episode_length_buf % int(self.cfg.commands.resampling_time / self.dt)==0).nonzero(as_tuple=False).flatten()
```

